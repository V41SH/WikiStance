{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T10:57:47.297253900Z",
     "start_time": "2025-06-19T10:57:11.310729900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\WebScience\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import preprocessor as p \n",
    "import re\n",
    "import json\n",
    "import wordninja\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import AutoModel, BertForMaskedLM, AdamW\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, BertweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:27:15.942084600Z",
     "start_time": "2025-06-08T20:27:15.926499400Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "def load_data(filename):\n",
    "\n",
    "    filename = [filename]\n",
    "    concat_text = pd.DataFrame()\n",
    "    raw_text = pd.read_csv(filename[0],usecols=[0], encoding='ISO-8859-1')\n",
    "    raw_label = pd.read_csv(filename[0],usecols=[2], encoding='ISO-8859-1')\n",
    "    raw_target = pd.read_csv(filename[0],usecols=[1], encoding='ISO-8859-1')\n",
    "    label = pd.DataFrame.replace(raw_label,['FAVOR','NONE','AGAINST'], [1,2,0])\n",
    "    concat_text = pd.concat([raw_text, label, raw_target], axis=1)\n",
    "    concat_text = concat_text[concat_text.Stance != 2]\n",
    "    \n",
    "    return(concat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:27:15.957745500Z",
     "start_time": "2025-06-08T20:27:15.942084600Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "def data_clean(strings, norm_dict):\n",
    "    \n",
    "    p.set_options(p.OPT.URL,p.OPT.EMOJI,p.OPT.RESERVED)\n",
    "    clean_data = p.clean(strings)  # using lib to clean URL, emoji...\n",
    "    clean_data = re.sub(r\"#SemST\", \"\", clean_data)\n",
    "    clean_data = re.findall(r\"[A-Za-z#@]+|[,.!?&/\\<>=$]|[0-9]+\",clean_data)\n",
    "    clean_data = [[x.lower()] for x in clean_data]\n",
    "    \n",
    "    for i in range(len(clean_data)):\n",
    "        if clean_data[i][0] in norm_dict.keys():\n",
    "            clean_data[i][0] = norm_dict[clean_data[i][0]]\n",
    "            continue\n",
    "        if clean_data[i][0].startswith(\"#\") or clean_data[i][0].startswith(\"@\"):\n",
    "            clean_data[i] = wordninja.split(clean_data[i][0]) # split compound hashtags\n",
    "    clean_data = [j for i in clean_data for j in i]\n",
    "\n",
    "    return clean_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:27:15.973329100Z",
     "start_time": "2025-06-08T20:27:15.957745500Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Clean All Data\n",
    "\n",
    "def clean_all(filename, norm_dict):\n",
    "    \n",
    "    concat_text = load_data(filename)\n",
    "    raw_data = concat_text['Tweet'].values.tolist() \n",
    "    label = concat_text['Stance'].values.tolist()\n",
    "    x_target = concat_text['Target'].values.tolist()\n",
    "    clean_data = [None for _ in range(len(raw_data))]\n",
    "    \n",
    "    for i in range(len(raw_data)):\n",
    "        clean_data[i] = data_clean(raw_data[i], norm_dict)\n",
    "        x_target[i] = data_clean(x_target[i], norm_dict)\n",
    "    \n",
    "    return clean_data,label,x_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T11:25:33.999119200Z",
     "start_time": "2025-06-19T11:25:33.956809600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "def convert_data_to_ids(tokenizer, target, text, window = 512, stride = 256):\n",
    "    \n",
    "    input_ids, seg_ids, attention_masks, sent_len, sample_map = [], [], [], [], []\n",
    "    # max_len = max(\n",
    "    # len(tokenizer.encode(' '.join(tar), ' '.join(sent), add_special_tokens=True))\n",
    "    # for tar, sent in zip(target, text)\n",
    "    # )\n",
    "    for doc_idx, (tar, sent) in enumerate(zip(target, text)):\n",
    "        enc = tokenizer.encode_plus(\n",
    "                            ' '.join(tar),                  # Target to encode\n",
    "                            ' '.join(sent),                 # Sentence to encode\n",
    "                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = window,               # Pad & truncate all sentences\n",
    "                            padding = 'max_length',\n",
    "                            truncation = 'only_second', \n",
    "                            stride = stride, \n",
    "                            return_overflowing_tokens=True,\n",
    "                            return_attention_mask = True,   # Construct attention masks\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        n_chunks = len(enc[\"input_ids\"])\n",
    "        input_ids.append(enc['input_ids'])\n",
    "        seg_ids.append(enc['token_type_ids'])\n",
    "        attention_masks.append(enc['attention_mask'])\n",
    "        sent_len.extend([sum(enc['attention_mask'])])\n",
    "        sample_map.extend([doc_idx] * n_chunks)\n",
    "    \n",
    "    return input_ids, seg_ids, attention_masks, sent_len, sample_map\n",
    "    \n",
    "def data_helper_bert(x_train_all,x_val_all,x_test_all,model_select):\n",
    "    \n",
    "    print('Loading data')\n",
    "    \n",
    "    x_train,y_train,x_train_target = x_train_all[0],x_train_all[1],x_train_all[2]                                                \n",
    "    x_val,y_val,x_val_target = x_val_all[0],x_val_all[1],x_val_all[2]\n",
    "    x_test,y_test,x_test_target = x_test_all[0],x_test_all[1],x_test_all[2]\n",
    "                                                         \n",
    "    print(\"Length of x_train: %d, the sum is: %d\"%(len(x_train), sum(y_train)))\n",
    "    print(\"Length of x_val: %d, the sum is: %d\"%(len(x_val), sum(y_val)))\n",
    "    print(\"Length of x_test: %d, the sum is: %d\"%(len(x_test), sum(y_test)))\n",
    "    \n",
    "    # get the tokenizer\n",
    "    if model_select == 'Bertweet':\n",
    "        tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "    elif model_select == 'Bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "        \n",
    "    # tokenization\n",
    "    x_train_input_ids, x_train_seg_ids, x_train_atten_masks, x_train_len = \\\n",
    "                    convert_data_to_ids(tokenizer, x_train_target, x_train)\n",
    "    x_val_input_ids, x_val_seg_ids, x_val_atten_masks, x_val_len = \\\n",
    "                    convert_data_to_ids(tokenizer, x_val_target, x_val)\n",
    "    x_test_input_ids, x_test_seg_ids, x_test_atten_masks, x_test_len = \\\n",
    "                    convert_data_to_ids(tokenizer, x_test_target, x_test)\n",
    "#     print(x_test_input_ids[0])\n",
    "    x_train_all = [x_train_input_ids,x_train_seg_ids,x_train_atten_masks,y_train,x_train_len]\n",
    "    x_val_all = [x_val_input_ids,x_val_seg_ids,x_val_atten_masks,y_val,x_val_len]\n",
    "    x_test_all = [x_test_input_ids,x_test_seg_ids,x_test_atten_masks,y_test,x_test_len]\n",
    "    \n",
    "    return x_train_all,x_val_all,x_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2025-06-19T10:57:58.653033200Z",
     "start_time": "2025-06-19T10:57:58.636886Z"
    }
   },
   "outputs": [],
   "source": [
    "# BERT/BERTweet\n",
    "\n",
    "class stance_classifier(nn.Module):\n",
    "\n",
    "    def __init__(self,num_labels,model_select):\n",
    "\n",
    "        super(stance_classifier, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        if model_select == 'Bertweet':\n",
    "            self.bert = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        elif model_select == 'Bert':\n",
    "            self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, x_input_ids, x_seg_ids, x_atten_masks, x_len):\n",
    "        \n",
    "        last_hidden = self.bert(input_ids=x_input_ids, \\\n",
    "                                attention_mask=x_atten_masks, token_type_ids=x_seg_ids, \\\n",
    "                               )\n",
    "        \n",
    "        query = last_hidden[0][:,0]\n",
    "        query = self.dropout(query)\n",
    "        \n",
    "        linear = self.relu(self.linear(query))\n",
    "        out = self.out(linear)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:27:16.020382400Z",
     "start_time": "2025-06-08T20:27:16.004553Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def compute_f1(preds, y):\n",
    "    \n",
    "    rounded_preds = F.softmax(preds)\n",
    "    _, indices = torch.max(rounded_preds, 1)\n",
    "                \n",
    "    correct = (indices == y).float()\n",
    "    acc = correct.sum()/len(correct) # compute accuracy\n",
    "    \n",
    "    y_pred = np.array(indices.cpu().numpy())\n",
    "    y_true = np.array(y.cpu().numpy())\n",
    "    result = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0,1])\n",
    "#     print(result[2][0],result[2][1])\n",
    "    f1_average = (result[2][0]+result[2][1])/2 # average F1 score of Favor and Against\n",
    "        \n",
    "    return acc, f1_average, result[0], result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T10:58:05.526362700Z",
     "start_time": "2025-06-19T10:58:05.477050900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main \n",
    "\n",
    "def data_loader(x_all, batch_size, data_type):\n",
    "    \n",
    "    x_input_ids = torch.tensor(x_all[0], dtype=torch.long).cuda()\n",
    "    x_seg_ids = torch.tensor(x_all[1], dtype=torch.long).cuda()\n",
    "    x_atten_masks = torch.tensor(x_all[2], dtype=torch.long).cuda()\n",
    "    # y = torch.tensor(x_all[3], dtype=torch.long).cuda()\n",
    "    x_len = torch.tensor(x_all[3], dtype=torch.long).cuda()\n",
    "\n",
    "    tensor_loader = TensorDataset(x_input_ids,x_seg_ids,x_atten_masks,x_len)\n",
    "    if data_type == 'train':\n",
    "        data_loader = DataLoader(tensor_loader, shuffle=True, batch_size=batch_size)\n",
    "    else:\n",
    "        data_loader = DataLoader(tensor_loader, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    return x_input_ids, x_seg_ids, x_atten_masks, x_len, data_loader\n",
    "    \n",
    "def sep_test_set(input_data):\n",
    "    \n",
    "    # split the combined test set for Trump, Biden and Bernie\n",
    "    data_list = [input_data[:777], input_data[777:1522], input_data[1522:2157]]\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def run_classifier(input_word_pair,model_select,train_mode):\n",
    "    \n",
    "    random_seeds = [0,1,14,15,16,17,19]\n",
    "    target_word_pair = input_word_pair\n",
    "    \n",
    "    #Creating Normalization Dictionary\n",
    "    with open(\"../source/noslang_data.json\", \"r\") as f:\n",
    "        data1 = json.load(f)\n",
    "    data2 = {}\n",
    "    with open(\"../source/emnlp_dict.txt\",\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            row = line.split('\\t')\n",
    "            data2[row[0]] = row[1].rstrip()\n",
    "    normalization_dict = {**data1,**data2}\n",
    "\n",
    "    for target_index in range(len(target_word_pair)):\n",
    "        best_result, best_val = [], []\n",
    "        for seed in random_seeds:    \n",
    "            print(\"current random seed: \", seed)\n",
    "\n",
    "            if train_mode == \"unified\":\n",
    "                filename1 = '../Dataset/raw_train_all.csv'\n",
    "                filename2 = '../Dataset/raw_val_all.csv'\n",
    "                filename3 = '../Dataset/raw_test_all.csv'\n",
    "            elif train_mode == \"adhoc\":\n",
    "                filename1 = '../Dataset/raw_train_'+target_word_pair[target_index]+'.csv'\n",
    "                filename2 = '../Dataset/raw_val_'+target_word_pair[target_index]+'.csv'\n",
    "                filename3 = '../Dataset/raw_test_'+target_word_pair[target_index]+'.csv'\n",
    "            x_train,y_train,x_train_target = clean_all(filename1, normalization_dict)\n",
    "            x_val,y_val,x_val_target = clean_all(filename2, normalization_dict)\n",
    "            x_test,y_test,x_test_target = clean_all(filename3, normalization_dict)\n",
    "                \n",
    "            num_labels = len(set(y_train))\n",
    "#             print(x_train_target[0])\n",
    "            x_train_all = [x_train,y_train,x_train_target]\n",
    "            x_val_all = [x_val,y_val,x_val_target]\n",
    "            x_test_all = [x_test,y_test,x_test_target]\n",
    "            \n",
    "            # set up the random seed\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed) \n",
    "\n",
    "            # prepare for model\n",
    "            x_train_all,x_val_all,x_test_all = data_helper_bert(x_train_all,x_val_all,x_test_all,model_select)\n",
    "#             print(x_test_all[0][0])\n",
    "            x_train_input_ids, x_train_seg_ids, x_train_atten_masks, y_train, x_train_len, trainloader = \\\n",
    "                                        data_loader(x_train_all, batch_size, 'train')\n",
    "            x_val_input_ids, x_val_seg_ids, x_val_atten_masks, y_val, x_val_len, valloader = \\\n",
    "                                        data_loader(x_val_all, batch_size, 'val')                            \n",
    "            x_test_input_ids, x_test_seg_ids, x_test_atten_masks, y_test, x_test_len, testloader = \\\n",
    "                                        data_loader(x_test_all, batch_size, 'test')\n",
    "\n",
    "            model = stance_classifier(num_labels,model_select).cuda()\n",
    "\n",
    "            for n,p in model.named_parameters():\n",
    "                if \"bert.embeddings\" in n:\n",
    "                    p.requires_grad = False\n",
    "            optimizer_grouped_parameters = [\n",
    "                {'params': [p for n, p in model.named_parameters() if n.startswith('bert.encoder')] , 'lr': lr},\n",
    "                {'params': [p for n, p in model.named_parameters() if n.startswith('bert.pooler')] , 'lr': 1e-3},\n",
    "                {'params': [p for n, p in model.named_parameters() if n.startswith('linear')], 'lr': 1e-3},\n",
    "                {'params': [p for n, p in model.named_parameters() if n.startswith('out')], 'lr': 1e-3}\n",
    "                ]\n",
    "            \n",
    "            loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "            optimizer = AdamW(optimizer_grouped_parameters)\n",
    "            \n",
    "            sum_loss = []\n",
    "            sum_val = []\n",
    "            train_f1_average = []\n",
    "            val_f1_average = []\n",
    "            if train_mode == \"unified\":\n",
    "                test_f1_average = [[] for i in range(3)]\n",
    "            elif train_mode == \"adhoc\":\n",
    "                test_f1_average = [[]]\n",
    "\n",
    "            for epoch in range(0, total_epoch):\n",
    "                print('Epoch:', epoch)\n",
    "                train_loss, valid_loss = [], []\n",
    "                model.train()\n",
    "                for input_ids,seg_ids,atten_masks,target,length in trainloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    output1 = model(input_ids, seg_ids, atten_masks, length)\n",
    "                    loss = loss_function(output1, target)\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                    optimizer.step()\n",
    "                    train_loss.append(loss.item())\n",
    "                sum_loss.append(sum(train_loss)/len(x_train))  \n",
    "                print(sum_loss[epoch])\n",
    "\n",
    "                # evaluation on dev set\n",
    "                model.eval()\n",
    "                val_preds = []\n",
    "                with torch.no_grad():\n",
    "                    for input_ids,seg_ids,atten_masks,target,length in valloader: \n",
    "                        pred1 = model(input_ids, seg_ids, atten_masks, length) \n",
    "                        val_preds.append(pred1)\n",
    "                pred1 = torch.cat(val_preds, 0)\n",
    "                acc, f1_average, precision, recall = compute_f1(pred1,y_val)\n",
    "                val_f1_average.append(f1_average)\n",
    "                \n",
    "                # evaluation on test set\n",
    "                with torch.no_grad():\n",
    "                    test_preds = []\n",
    "                    for input_ids,seg_ids,atten_masks,target,length in testloader:\n",
    "                        pred1 = model(input_ids, seg_ids, atten_masks, length)\n",
    "                        test_preds.append(pred1)\n",
    "                    pred1 = torch.cat(test_preds, 0)\n",
    "                    if train_mode == \"unified\":\n",
    "                        pred1_list = sep_test_set(pred1)\n",
    "                        y_test_list = sep_test_set(y_test)\n",
    "                    else:\n",
    "                        pred1_list = [pred1]\n",
    "                        y_test_list = [y_test]\n",
    "                        \n",
    "                    for ind in range(len(y_test_list)):\n",
    "                        pred1 = pred1_list[ind]\n",
    "                        acc, f1_average, precision, recall = compute_f1(pred1,y_test_list[ind])\n",
    "                        test_f1_average[ind].append(f1_average)\n",
    "            \n",
    "            best_epoch = [index for index,v in enumerate(val_f1_average) if v == max(val_f1_average)][-1] \n",
    "            best_result.append([f1[best_epoch] for f1 in test_f1_average])\n",
    "\n",
    "            print(\"******************************************\")\n",
    "            print(\"dev results with seed {} on all epochs\".format(seed))\n",
    "            print(val_f1_average)\n",
    "            best_val.append(val_f1_average[best_epoch])\n",
    "            print(\"******************************************\")\n",
    "            print(\"test results with seed {} on all epochs\".format(seed))\n",
    "            print(test_f1_average)\n",
    "            print(\"******************************************\")\n",
    "        \n",
    "        # model that performs best on the dev set is evaluated on the test set\n",
    "        print(\"model performance on the test set: \")\n",
    "        print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T23:57:37.093667300Z",
     "start_time": "2025-06-08T22:49:59.427130900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current random seed:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\3101927427.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  label = pd.DataFrame.replace(raw_label,['FAVOR','NONE','AGAINST'], [1,2,0])\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\3101927427.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  label = pd.DataFrame.replace(raw_label,['FAVOR','NONE','AGAINST'], [1,2,0])\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\3101927427.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  label = pd.DataFrame.replace(raw_label,['FAVOR','NONE','AGAINST'], [1,2,0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Length of x_train: 17224, the sum is: 8347\n",
      "Length of x_val: 2193, the sum is: 1052\n",
      "Length of x_test: 2157, the sum is: 1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\WebScience\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5676999343511563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46931661238693373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4475349178812121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n",
      "C:\\Users\\meenakshi\\AppData\\Local\\Temp\\ipykernel_19196\\998712729.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rounded_preds = F.softmax(preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************\n",
      "dev results with seed 0 on all epochs\n",
      "[np.float64(0.7652703609227975), np.float64(0.7801855795305196), np.float64(0.7847084369826202)]\n",
      "******************************************\n",
      "test results with seed 0 on all epochs\n",
      "[[np.float64(0.7629840848806366), np.float64(0.7522854925263518), np.float64(0.7663891709872255)], [np.float64(0.7661167532610231), np.float64(0.7673881625319476), np.float64(0.7623042505592841)], [np.float64(0.7895407050778865), np.float64(0.8029913055564519), np.float64(0.8014324855560189)]]\n",
      "******************************************\n",
      "model performance on the test set: \n",
      "[[np.float64(0.7663891709872255), np.float64(0.7623042505592841), np.float64(0.8014324855560189)]]\n",
      "Saved model to ../Dataset/trained_model/all_seed0_epoch2.pt\n"
     ]
    }
   ],
   "source": [
    "# run classifier in unified setting\n",
    "\n",
    "lr = 2e-5\n",
    "batch_size = 32\n",
    "total_epoch = 3\n",
    "run_classifier(['all'],'Bert','unified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-21T18:04:08.853902800Z",
     "start_time": "2025-06-21T18:04:00.894540700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "512\n",
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                               Tweet  \\\n0  --- \\n+++ \\n@@ -2,31 +2,31 @@\\n {{use American...   \n1  --- \\n+++ \\n@@ -1,44 +1,44 @@\\n {{Current elec...   \n2  --- \\n+++ \\n@@ -506,42 +506,42 @@\\n ! style=\"w...   \n3  --- \\n+++ \\n@@ -462,50 +462,55 @@\\n  | candida...   \n4  --- \\n+++ \\n@@ -2147,38 +2147,38 @@\\n | candid...   \n\n                        Time Predicted_Stance  \n0  2020-11-04 18:01:34+00:00            FAVOR  \n1  2020-11-04 05:27:59+00:00          AGAINST  \n2  2020-11-04 03:57:53+00:00          AGAINST  \n3  2020-11-04 15:55:19+00:00          AGAINST  \n4  2020-11-04 17:58:56+00:00          AGAINST  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet</th>\n      <th>Time</th>\n      <th>Predicted_Stance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>--- \\n+++ \\n@@ -2,31 +2,31 @@\\n {{use American...</td>\n      <td>2020-11-04 18:01:34+00:00</td>\n      <td>FAVOR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>--- \\n+++ \\n@@ -1,44 +1,44 @@\\n {{Current elec...</td>\n      <td>2020-11-04 05:27:59+00:00</td>\n      <td>AGAINST</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>--- \\n+++ \\n@@ -506,42 +506,42 @@\\n ! style=\"w...</td>\n      <td>2020-11-04 03:57:53+00:00</td>\n      <td>AGAINST</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>--- \\n+++ \\n@@ -462,50 +462,55 @@\\n  | candida...</td>\n      <td>2020-11-04 15:55:19+00:00</td>\n      <td>AGAINST</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>--- \\n+++ \\n@@ -2147,38 +2147,38 @@\\n | candid...</td>\n      <td>2020-11-04 17:58:56+00:00</td>\n      <td>AGAINST</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, pandas as pd\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "# target = \"Donald Trump\"\n",
    "# target = \"Bernie Sanders\"\n",
    "target = \"Joe Biden\"\n",
    "folder_name = \"election\"\n",
    "\n",
    "df = pd.read_csv(\"../Dataset/election.csv\") \n",
    "df = df[[\"Tweet\", \"Time\"]]                      \n",
    "\n",
    "tokenizer   = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                            do_lower_case=True)\n",
    "LABEL_MAP   = {0: \"AGAINST\", 1: \"FAVOR\"}\n",
    "\n",
    "tweets  = df[\"Tweet\"].astype(str).str.split().tolist()\n",
    "targets = [target.split()] * len(tweets)\n",
    "\n",
    "ids, segs, masks, slen, sample_map = convert_data_to_ids(tokenizer, targets, tweets)\n",
    "print(type(ids))          # list\n",
    "print(type(ids[0]))       # list or tensor?\n",
    "print(len(ids[0]))        # 15301  (way too large)\n",
    "print(len(ids[1]))  \n",
    "test_all = [ids, segs, masks, slen]     # (N,)\n",
    "\n",
    "# dummy label vector so the dataset shape matches what your model expects\n",
    "# dummy_labels = torch.zeros(len(input_ids), dtype=torch.long)\n",
    "x_test_input_ids, x_test_seg_ids, x_test_atten_masks, x_test_len, testloader = data_loader(test_all, batch_size = 64, data_type='test')\n",
    "# dataset = TensorDataset(input_ids,\n",
    "#                         token_type,\n",
    "#                         attn_mask,\n",
    "#                         dummy_labels,          # dummy labels\n",
    "#                         seq_len)\n",
    "# loader  = DataLoader(dataset,\n",
    "#                      sampler=SequentialSampler(dataset),\n",
    "#                      batch_size=64,                     # any value\n",
    "#                      shuffle=False)\n",
    "\n",
    "model = stance_classifier(num_labels=len(LABEL_MAP), model_select=\"Bert\")\n",
    "# model.load_state_dict(torch.load(\"../Dataset/trained_model/all_seed0_epoch2.pt\", map_location=\"cuda\", weights_only=False))\n",
    "ckpt = torch.load(\"../Dataset/trained_model/all_seed0_epoch2.pt\", map_location=\"cuda\", weights_only=False)\n",
    "model_weight = ckpt[\"model_state_dict\"]\n",
    "model.load_state_dict(model_weight)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "pred = []\n",
    "with torch.no_grad():\n",
    "    for b_ids, b_segs, b_masks, b_len in testloader:\n",
    "        b_ids, b_segs, b_masks, b_len = \\\n",
    "            (t.to(device) for t in (b_ids, b_segs, b_masks, b_len))\n",
    "        logits = model(b_ids, b_segs, b_masks, b_len)     # forward()\n",
    "        pred.append(logits.cpu())\n",
    "        # pred.extend(logits.argmax(dim=-1).cpu().tolist())\n",
    "logits_all  = torch.cat(pred, dim=0)  \n",
    "doc_logits  = torch.zeros(len(df), logits_all.size(1))\n",
    "for lgt, doc_idx in zip(logits_all, sample_map):\n",
    "    doc_logits[doc_idx] += lgt\n",
    "doc_logits /= torch.bincount(torch.tensor(sample_map)).unsqueeze(1)  # mean\n",
    "pred = doc_logits.argmax(dim=-1).tolist()\n",
    "df[\"Predicted_Stance\"] = [LABEL_MAP[i] for i in pred]\n",
    "out_cols = [\"Tweet\", \"Time\", \"Predicted_Stance\"]  \n",
    "os.makedirs(f\"final/{folder_name}/\", exist_ok=True)\n",
    "df.to_csv(f\"final/{folder_name}/{target}_results.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
